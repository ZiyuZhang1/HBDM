{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_stringdb = 'D:/study/thesis/project/HBDM-main/data/nn_data/stringdb/'\n",
    "# load local STRING database and names\n",
    "df = pd.read_csv(local_stringdb+'9606.protein.info.v12.0.txt', sep='\\t', header=0, usecols=['#string_protein_id', 'preferred_name'])\n",
    "df['preferred_name'] = df['preferred_name'].str.upper()\n",
    "stringId2name = df.set_index('#string_protein_id')['preferred_name'].to_dict()\n",
    "name2stringId = df.set_index('preferred_name')['#string_protein_id'].to_dict()\n",
    "df = pd.read_csv(local_stringdb+'9606.protein.aliases.v12.0.txt', sep='\\t', header=0, usecols=['#string_protein_id', 'alias']).drop_duplicates(['alias'], keep='first')\n",
    "df['alias'] = df['alias'].str.upper()\n",
    "aliases2stringId = df.set_index('alias')['#string_protein_id'].to_dict()\n",
    "\n",
    "#string_score_transform = lambda x: -np.log(x/1000)\n",
    "\n",
    "graph_df = pd.read_csv(local_stringdb+'9606.protein.physical.links.detailed.v12.0.txt', sep=' ', header=0).convert_dtypes().replace(0, float('nan'))\n",
    "#network['combined_score'] = network['combined_score'].apply(string_score_transform)\n",
    "graph_df = graph_df[['protein1', 'protein2','combined_score']]\n",
    "G = nx.from_paandas_edgelist(graph_df, source='protein1', target='protein2', edge_attr='combined_score', create_using=nx.Graph)\n",
    "print(nx.is_connected(G))\n",
    "components = list(nx.connected_components(G))\n",
    "# Print information about each connected component\n",
    "# for i, component in enumerate(components):\n",
    "#     print(f\"Component {i + 1}\")\n",
    "\n",
    "#     # Extract the edges for each component\n",
    "#     subgraph = G.subgraph(component)\n",
    "#     component_edges = subgraph.edges()\n",
    "#     print('nodes',len(subgraph.nodes),'Edges:',len(subgraph.edges))\n",
    "\n",
    "subgraph = G.subgraph(components[0])\n",
    "graph_df = nx.to_pandas_edgelist(subgraph, source='protein1', target='protein2')\n",
    "proteins = sorted(list(set(graph_df['protein1'].tolist())|set(graph_df['protein2'].tolist())))\n",
    "gene2node = {value: index for index, value in enumerate(proteins)}\n",
    " \n",
    "file_path = r'D:\\study\\thesis\\project\\HBDM-main\\data\\datasets\\ppi\\ppi_index.pkl'\n",
    "# Serialize and save the Tensor to the file\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(gene2node, file)\n",
    "# Close the file\n",
    "file.close()\n",
    "\n",
    "graph_df['node1']=graph_df['protein1'].map(gene2node)\n",
    "graph_df['node2']=graph_df['protein2'].map(gene2node)\n",
    "G = nx.from_pandas_edgelist(graph_df, source='node1', target='node2', edge_attr='combined_score', create_using=nx.Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disgenet_save = r'D:\\study\\thesis\\project\\HBDM-main\\data\\disease\\cad_node.pkl'\n",
    "\n",
    "def convert_stringId(alias):\n",
    "    try:\n",
    "        stringId = name2stringId[alias]\n",
    "    except:\n",
    "        #print(alias, 'can\\'t be converted by name2stringId! Now trying aliases2stringId.')\n",
    "        try:\n",
    "            stringId = aliases2stringId[alias]\n",
    "        except:\n",
    "            #print(alias, 'can\\'t be converted by aliases2stringId! Now return None.')\n",
    "            stringId = None\n",
    "    #print(alias, stringId)\n",
    "    return stringId\n",
    "protein_names = list(aliases2stringId.keys())\n",
    "protein_names.extend(list(name2stringId.keys()))\n",
    "\n",
    "ppi_index = gene2node\n",
    "node2string = {value: key for key, value in ppi_index.items()}\n",
    "\n",
    "humans = set(ppi_index.keys())\n",
    "\n",
    "caddf = pd.read_csv(r'D:\\study\\thesis\\project\\HBDM-main\\data\\disease\\Coronary_artery_disease.tsv',sep='\\t')\n",
    "cadlist = caddf['Gene'].tolist()\n",
    "group_node = []\n",
    "for gene in cadlist:\n",
    "    if gene in protein_names:\n",
    "        stringid = convert_stringId(gene)\n",
    "        if stringid in humans:\n",
    "            node = ppi_index[stringid]\n",
    "            group_node.append(node)\n",
    "with open(disgenet_save, 'wb') as file:\n",
    "    pickle.dump(group_node, file)\n",
    "# Close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df.to_csv(r'D:\\study\\thesis\\project\\HBDM-main\\data\\ppi_connect.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = G.nodes()\n",
    "\n",
    "# Get all possible edges in the complete graph\n",
    "all_possible_edges = [(u, v) for u in nodes for v in nodes if u != v]\n",
    "\n",
    "# Get the existing edges in the graph\n",
    "existing_edges = list(G.edges())\n",
    "\n",
    "part_edges = sample(all_possible_edges,int(len(existing_edges)*1.3))\n",
    "non_existing_edges = list(set(part_edges) - set(existing_edges))\n",
    "selected_non_exist_edges = sample(non_existing_edges,int(len(existing_edges)*0.3))\n",
    "\n",
    "del all_possible_edges \n",
    "del part_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.minimum_spanning_tree(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pool = list(set(existing_edges)-set(H.edges()))\n",
    "mask_edegs = sample(sample_pool, int(len(existing_edges)*0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.array([(u, v, data['combined_score']) for u, v, data in G.edges(data=True)])\n",
    "\n",
    "# Determine i, j, and weights\n",
    "i = np.where(edges[:, 0] > edges[:, 1], edges[:, 1], edges[:, 0])\n",
    "j = np.where(edges[:, 0] > edges[:, 1], edges[:, 0], edges[:, 1])\n",
    "weights = edges[:, 2]\n",
    "weights = weights*0.001\n",
    "\n",
    "root = 'D:/study/thesis/project/HBDM-main/data/datasets/ppi/'\n",
    "np.savetxt(root+'sparse_i.txt', np.array(i), delimiter='\\n')\n",
    "np.savetxt(root+'sparse_j.txt', np.array(j), delimiter='\\n')\n",
    "np.savetxt(root+'sparse_w.txt', np.array(weights), delimiter='\\n')\n",
    "\n",
    "weights = (weights*0.01).astype(int)\n",
    "np.savetxt(root+'sparse_10.txt', np.array(weights), delimiter='\\n')\n",
    "\n",
    "level_edges = dict()\n",
    "for u, v, data in G.edges(data=True):\n",
    "    level = int(str(data['combined_score'])[0])\n",
    "    if level in level_edges:\n",
    "        level_edges[level].append([u, v])\n",
    "    else:\n",
    "        level_edges[level]=[[u, v]]\n",
    "\n",
    "for level in level_edges:\n",
    "    edges = np.array(level_edges[level])\n",
    "    sparse_i = np.where(edges[:, 0] > edges[:, 1], edges[:, 1], edges[:, 0])\n",
    "    sparse_j = np.where(edges[:, 0] > edges[:, 1], edges[:, 0], edges[:, 1])\n",
    "    np.savetxt(root+'level_'+str(level)+'_sparse_i.txt', np.array(sparse_i), delimiter='\\n')\n",
    "    np.savetxt(root+'level_'+str(level)+'_sparse_j.txt', np.array(sparse_j), delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "738800"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ppi link prediction train graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "traingraph = G.edge_subgraph(list(set(existing_edges)-set(mask_edegs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517160"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traingraph.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.array([(u, v, data['combined_score']) for u, v, data in traingraph.edges(data=True)])\n",
    "\n",
    "# Determine i, j, and weights\n",
    "i = np.where(edges[:, 0] > edges[:, 1], edges[:, 1], edges[:, 0])\n",
    "j = np.where(edges[:, 0] > edges[:, 1], edges[:, 0], edges[:, 1])\n",
    "weights = edges[:, 2]\n",
    "weights = weights*0.001\n",
    "\n",
    "root = 'D:/study/thesis/project/HBDM-main/data/datasets/ppi_linkpredict/'\n",
    "np.savetxt(root+'sparse_i.txt', np.array(i), delimiter='\\n')\n",
    "np.savetxt(root+'sparse_j.txt', np.array(j), delimiter='\\n')\n",
    "np.savetxt(root+'sparse_w.txt', np.array(weights), delimiter='\\n')\n",
    "\n",
    "weights = (weights*0.01).astype(int)\n",
    "np.savetxt(root+'sparse_10.txt', np.array(weights), delimiter='\\n')\n",
    "\n",
    "level_edges = dict()\n",
    "for u, v, data in traingraph.edges(data=True):\n",
    "    level = int(str(data['combined_score'])[0])\n",
    "    if level in level_edges:\n",
    "        level_edges[level].append([u, v])\n",
    "    else:\n",
    "        level_edges[level]=[[u, v]]\n",
    "\n",
    "for level in level_edges:\n",
    "    edges = np.array(level_edges[level])\n",
    "    sparse_i = np.where(edges[:, 0] > edges[:, 1], edges[:, 1], edges[:, 0])\n",
    "    sparse_j = np.where(edges[:, 0] > edges[:, 1], edges[:, 0], edges[:, 1])\n",
    "    np.savetxt(root+'level_'+str(level)+'_sparse_i.txt', np.array(sparse_i), delimiter='\\n')\n",
    "    np.savetxt(root+'level_'+str(level)+'_sparse_j.txt', np.array(sparse_j), delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221640"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mask_edegs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.array(mask_edegs)\n",
    "\n",
    "# Determine i, j, and weights\n",
    "i = np.where(edges[:, 0] > edges[:, 1], edges[:, 1], edges[:, 0])\n",
    "j = np.where(edges[:, 0] > edges[:, 1], edges[:, 0], edges[:, 1])\n",
    "\n",
    "root = 'D:/study/thesis/project/HBDM-main/data/datasets/ppi_linkpredict/'\n",
    "np.savetxt(root+'sparse_i_rem.txt', np.array(i), delimiter='\\n')\n",
    "np.savetxt(root+'sparse_j_rem.txt', np.array(j), delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221640"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_non_exist_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = np.array(selected_non_exist_edges)\n",
    "\n",
    "# Determine i, j, and weights\n",
    "i = np.where(edges[:, 0] > edges[:, 1], edges[:, 1], edges[:, 0])\n",
    "j = np.where(edges[:, 0] > edges[:, 1], edges[:, 0], edges[:, 1])\n",
    "\n",
    "root = 'D:/study/thesis/project/HBDM-main/data/datasets/ppi_linkpredict/'\n",
    "np.savetxt(root+'non_sparse_i.txt', np.array(i), delimiter='\\n')\n",
    "np.savetxt(root+'non_sparse_j.txt', np.array(j), delimiter='\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbdm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
